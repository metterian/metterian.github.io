---
layout: post
title:  "ğŸ¦„ How to build a State-of-the-Art Conversational AI"
author: "metterian"
tags: AI NLP
---



# How to build a State-of-the-Art Conversational AI with Transfer Learning

ì´ ê¸€ì€ **ConvAI2  NeurIPS**(2018) ëŒ€íšŒì—ì„œ **SOTA**(state-of-the-art)ë¥¼ ê¸°ë¡í•œ **Hugging Face** ì˜ Conversation AIì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ë¥¼ ë²ˆì—­í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì¡¸ì—… í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ëŠ” í•™ë¶€ìƒ ìˆ˜ì¤€ì—ì„œ ì‘ì„±í•œ ê¸€ì´ë‹ˆ ì°¸ê³ í•˜ê³  ë´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.ğŸ˜ƒ 



> ë‹¤ìŒ ì‚¬ì´íŠ¸ì—ì„œ Hugging Faceê°€ ì œì‘í•œ ê°„ë‹¨í•œ ë°ëª¨ë¥¼ ì²´í—˜ í•´ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ğŸ®*[convai.huggingface.co](https://convai.huggingface.co/)*. 

![img](https://miro.medium.com/max/600/1*Fn0bcNyyEyqpGq-nCPyoYw.gif)



## ê¸€ì˜ ì£¼ìš” ëª©í‘œ

- OpenAIì¸ GPT ì™€ GPT-2 Transformer language ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì „ì´í•™ìŠµì„ í†µí•œ ìµœì ë‹¨ ëŒ€í™” ì—ì´ì „íŠ¸ë¥¼ ì œì‘
- [ConvAI2](http://convai.io/)ì—ì„œ ìš°ìŠ¹í•œ NeurIPS 2018 ëŒ€í™”ë¥¼ ì‚¬ìš©

> ì´ íŠœí† ë¦¬ì–¼ì— ëŒ€í•œ ìì„¸í•œ ì½”ë“œëŠ” ë‹¤ìŒ [Git ë ˆí¬ì§€í† ë¦¬](https://github.com/huggingface/transfer-learning-conv-ai)ì—ì„œ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



# Personalityë¥¼ ì§€ë‹Œ AI ğŸ¤ 

ë³¸ í”„ë¡œì íŠ¸ì€ ëª©ì ì€ **Persona** ë¥¼ ì§€ë‹Œ **Conversaional AI** ë¥¼ ì œì‘í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

ë³¸ í”„ë¡œì íŠ¸ì˜ ëŒ€í™” ì—ì´ì „íŠ¸(Dialog Agent)ëŠ” ì–´ë–¤ personaê°€ ì–´ë–¤ ëŒ€í™” ê¸°ë¡(History)ì„ ì„¤ëª…í•˜ëŠ” ì§€ì— ëŒ€í•œ Knowledge Baseë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìë¡œë¶€í„° ìƒˆë¡œìš´ ëŒ€í™”ê°€ ì…ë ¥ë˜ë©´ ëŒ€í™” ì—ì´ì „íŠ¸ëŠ” ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ Personaë¥¼ ì§€ë‹Œ ëŒ€í™”ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

í”„ë¡œì íŠ¸ì˜ ê³„íšì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![img](https://miro.medium.com/max/1035/1*sIRX4M3--Qrvo-RLqH-7GQ.png)



### ë”¥ ëŸ¬ë‹ìœ¼ë¡œ ëŒ€í™” ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµ ì‹œí‚¬ë•Œì˜ ë¬¸ì œì 

- ëŒ€í™” ë°ì´í„° ì…‹ì´ ì‘ê±°ë‚˜, ìœ ì°½í•˜ê±°ë‚˜ ê´€ë ¨ìˆëŠ” ë‹µë³€ì„ í•˜ê¸°ì— í•™ìŠµëŸ‰ì´ ë¶€ì¡±í•¨

  â†’ ì´ì— ëŒ€í•œ í•´ê²°ë°©ì•ˆìœ¼ë¡œ 'Smart Beam Search' ë°©ì‹ì´ ê³ ë ¤ ë˜ì§€ë§Œ í•´ë‹¹ ê¸€ì—ì„œëŠ” **'ì „íˆ í•™ìŠµ'**(transfer-learning) ì„ í™œìš©í–ˆë‹¤.

### í•´ê²°ì±…

- ê¸´ ì—°ì†ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡, ë§¤ìš° í° í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜(very large corpus of text)ì— ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ë¦¬ êµìœ¡í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•œë‹¤.
- **íŒŒì¸íŠœë‹**(Fine-Tune)ì„ í†µí•˜ì—¬ ëŒ€í™”ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•œë‹¤.

ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµ(Pretraining)ìœ¼ë¡œ êµ¬ì¶• í•˜ëŠ” ë°©ë²•ì€ ì‹œê°„ ë¹„ìš©ì´ ë§ì´ ì†Œìš”ë˜ê¸°ì—, ì˜¤í”ˆì†ŒìŠ¤ë¥¼ í™œìš©í•œë‹¤. ë˜í•œ ë°ì´í„° ì…‹ì´ í¬ê³ , ì¢‹ì€ ë°ì´í„°(*The bigger the better*)ë¥¼ ì‚¬ìš©í•˜ë©´ ì¢‹ì§€ë§Œ ë¬¸ì¥ ì¦‰, í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê²Œ ëª©ì ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ì‚¬ì „í•™ìŠµ(pretraining)ëœ NPL ëª¨ë¸ë¡œ BERTë¥¼ ë§ì´ ì‚¬ìš©í•˜ì§€ë§Œ, ì™„ë²½ ë¬¸ì¥(Maskingì´ ì—†ëŠ” ë¬¸ì¥)ì—ì„œëŠ” í•™ìŠµì´ ë˜ì—ˆì§€ë§Œ, unfinished sentences(Maskingì´ ìˆëŠ” ë¬¸ì¥)ì—ì„œëŠ” í•™ìŠµì´ ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— **GPT** & **GPT-2**ë¥¼ ì‚¬ìš©í–ˆë‹¤.



## ğŸ¦„ OpenAI GPT and GPT-2 models

2018ë…„ê³¼ 2019ë…„, ì•Œë ‰ ë˜ë“œí¬ë“œ, ì œí”„ë¦¬ ìš°, ì˜¤í”ˆì˜ ë™ë£Œë“¤AIëŠ” ë§¤ìš° ë§ì€ ì–‘ì˜ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ëœ ë‘ ê°€ì§€ ì–¸ì–´ ëª¨ë¸ì¸ GPTì™€ GPT-2(Generative Pre-trained Transformer) ìƒì„±í–ˆë‹¤.

GPTì™€ GPT-2ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•œë‹¤. ***decoder*** í˜¹ì€ ***causal*** ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì™¼ìª½ ë¬¸ë§¥ì„ ì‚¬ìš©í•œë‹¤.

![https://miro.medium.com/max/2077/1*YmND0Qj8O6b35J1yU_CPKQ.png](https://miro.medium.com/max/2077/1*YmND0Qj8O6b35J1yU_CPKQ.png)

Decoder/Casual íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì™¼ìª½ì—ì„œ ë¶€í„° ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±ì„ ì˜ˆì¸¡í•œë‹¤. [ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (Attention Mechanism)](https://www.notion.so/Attention-Mechanism-d6ede65d09ee4be9b20fc4f19e074d13) ì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ ë‹¤ìŒ ì‚¬ì´íŠ¸ì—ì„œ ìì„¸í•œ ë‚´ìš©ì„ ì°¾ì•„ ë³¼ ìˆ˜ ìˆë‹¤ .[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

ë³¸ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ, ì–¸ì–´ ëª¨ë¸(Language Model)ì€ ë‹¨ìˆœíˆ ì…ë ¥ ì‹œí€€ìŠ¤(Input Sequence)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ë¿ë§Œ ì•„ë‹ˆë¼, ì…ë ¥ ì‹œí€€ìŠ¤ ë‹¤ìŒì— ì´ì–´ì§€ëŠ” í† í°ì— ëŒ€í•œ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„± í•´ì•¼í–ˆë‹¤. ì–¸ì–´ ëª¨ë¸ì€ ëŒ€ê°œ ìœ„ì˜ ê·¸ë¦¼ì— í‘œì‹œëœ ê²ƒì²˜ëŸ¼ ê¸´ ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ê° í† í°ì„ ë”°ë¥´ëŠ” í† í°ì„ ì˜ˆì¸¡í•˜ì—¬ ë³‘ë ¬ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ëœë‹¤.

ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ì—ì„œ ì´ëŸ¬í•œ ëª¨ë¸ì„ ì‚¬ì „ êµìœ¡í•˜ëŠ” ê²ƒì€ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…ì´ê¸° ë•Œë¬¸ì— Open AIì—ì„œ Pre-trainedë  ëª¨ë¸ê³¼ í† í°ë¼ì´ì €ë¥¼ ì‚¬ìš©í–ˆë‹¤. TonkenizerëŠ” ì…ë ¥ ë¬¸ìì—´ì„ í† í°(ë‹¨ì–´/í•˜ìœ„ ë‹¨ì–´)ìœ¼ë¡œ ë¶„í• í•˜ê³ , ì´ëŸ¬í•œ í† í°ì„ ëª¨ë¸ ì–´íœ˜ë¥¼ ë³€í™˜í•œë‹¤.





## í”„ë¡œì íŠ¸ ëª©ì 

- í† í° ì‹œí€€ìŠ¤(a sequence of tokens)ë¥¼ ì¸í’‹ìœ¼ë¡œ ë°›ê³ ,
- **ì…ë ¥ ìˆœì„œì— ë”°ë¼ ë‹¤ìŒ í† í°ì˜ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¡œ ìƒì„±**
- ì–¸ì–´ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ê¸´ ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ê° í† í°ì„ ë”°ë¥´ëŠ” í† í°ì„ ì˜ˆì¸¡í•˜ì—¬ **ë³‘ë ¬ ë°©ì‹**ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤.

ë§ë­‰ì¹˜ê°€ í° ê²½ìš° í† í°í™”ì— ë§ì€ ë¹„ìš©ì´ ë°œìƒí•œë‹¤. ê·¸ë˜ì„œ OpenAIì—ì„œ ì‚¬ì „í•™ìŠµëœ tokenizerë¡œ ìš°ë¦¬ ëª¨ë¸ì— ì ìš©í•´ ë³´ì•˜ë‹¤.

[pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

```python
from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer

model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')
tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')
```

*OpenAI GPT Double Heads Model*ë¼ëŠ” ëª¨ë¸ë¥¼ ë¶ˆëŸ¬ì˜´



## ğŸ‘» ëŒ€í™”ì˜ì—­ì— ì–¸ì–´ ëª¨ë¸ ì ìš©

---

ìš°ë¦¬ ëª¨ë¸ì€ **ë‹¨ì¼ì…ë ¥(Single Input)** ì— í›ˆë ¨ë˜ì—ˆë‹¤: ì¼ë ¨ì˜ ë‹¨ì–´ë“¤(a sequence of words)

ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì—¬ëŸ¬ ìœ í˜•ì˜ **ì»¨í…ìŠ¤íŠ¸**(***Context***)ë¥¼ ì‚¬ìš©í•œë‹¤:

- í•œ ê°œ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ê°œì¸ ì„¤ì • ë¬¸ì¥,
- ì‚¬ìš©ìë¡œë¶€í„° ìµœì†Œí•œ ë§ˆì§€ë§‰ ë§ì„ í•œ ëŒ€í™” ê¸°ë¡,
- ì¶œë ¥ ì‹œí€€ìŠ¤ ë‹¨ì–´ë¥¼ ë‹¨ì–´ë³„ë¡œ(word by word) ìƒì„±í•œ ì´í›„ ì´ë¯¸ ìƒì„±ëœ ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ í† í°(?)

> ì–´ë–»ê²Œ ìœ„ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë§¥ë½(***Context***)ì„ ê³ ë ¤í•œ ì…ë ¥ì„ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œ?

ë¬¸ë§¥(Context) ì„¸ê·¸ë¨¼íŠ¸(ë¶€ë¶„)ë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ **ì—°ê²°(Concatenate)**ì‹œì¼œ, ê·¸ ëŒ€ë‹µì„ ë§ˆì§€ë§‰ì— ë†“ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ë‹¤ìŒ ì‹œí€€ìŠ¤ë¥¼ ê³„ì† ìƒì„±í•˜ì—¬ í† í°ìœ¼ë¡œ ì‘ë‹µ í† í° ìƒì„±í•  ìˆ˜ ìˆë‹¤.

![https://miro.medium.com/max/4711/1*RWEUB0ViLTdMjIQd61_WIg.png](https://miro.medium.com/max/4711/1*RWEUB0ViLTdMjIQd61_WIg.png)

(íŒŒë€ìƒ‰): ë³‘í•©ëœ í˜ë¥´ì†Œë‚˜, (í•‘í¬), (ë…¹ìƒ‰): ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬

### í•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ ë¬¸ì œì ì´ ì¡´ì¬:

- "*íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ìƒ‰ê¹”ë¡œ êµ¬ë¶„ í•˜ì§€ ëª»í•œë‹¤."* êµ¬ë¶„ í† í°(The delimiter tokens)ì€ ê° ë‹¨ì–´ê°€ ì–´ë–¤ ë¶€ë¶„ì— ì†í•˜ëŠ”ì§€ ê°„ë‹¨íˆ ì•Œë ¤ì¤€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "NYC"ë¼ëŠ” ë‹¨ì–´ëŠ” ìš°ë¦¬ì˜ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ìœ¼ë¡œ í‘œì‹œë˜ì§€ë§Œ, ìš°ë¦¬ì˜ ëª¨ë¸ì€ êµ¬ë¶„ì(delimiter)ë¡œë§Œ Personaì¸ì§€, Historyì¸ì§€ ì—¬ë¶€ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆë‹¤: ê·¸ëŸ¬ë¯€ë¡œ **ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤.**
- *"íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤"* ì—¬ê¸°ì„œ ì–´íƒ ì…˜(Attention)ì€ Dot-Product Attentionì„ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ **ìš°ë¦¬ëŠ” í† í° ë§ˆë‹¤ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€ í•´ì•¼í•œë‹¤.**
- Dot-Product Attentionì— ëŒ€í•œ ì •ë³´

    [ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ transfomer(self-attention)](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)

    [ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/22893)

ì´ë¥¼ í•´ê²° í•˜ê¸° ìœ„í•´ ë‹¨ì–´, ìœ„ì¹˜ ë° ì„¸ê·¸ë¨¼íŠ¸(***word, position and segments***)ì— ëŒ€í•´ ì…ë ¥ì„ ë°›ëŠ” 3ê°œì˜ ë³‘ë ¬ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ê³ ,  ì´ë“¤ì„ í•˜ë‚˜ì˜ ë‹¨ì¼ ì‹œí€€ìŠ¤(Single Sequence)ë¡œ ê²°í•©í•œë‹¤. ì¦‰, ë‹¨ì–´ ìœ„ì¹˜ ë° ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©ì˜ ì„¸ ê°€ì§€ ìœ í˜•ì„ í•©ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

![https://miro.medium.com/max/4711/1*r7vi6tho6sfpVx-ZQLPDUA.png](https://miro.medium.com/max/4711/1*r7vi6tho6sfpVx-ZQLPDUA.png)

## ì‹¤í–‰

---

ì²«ë²ˆì§¸ë¡œ, êµ¬ë¶„ ê¸°í˜¸ ë° ì„¸ê·¸ë¨¼íŠ¸ í‘œì‹œê¸°ì— *íŠ¹ìˆ˜ í† í°(special Token)*ì„ ì¶”ê°€í–ˆë‹¤. ì´ íŠ¹ìˆ˜ í† í°ì€ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ í¬í•¨ ë˜ì§€ ì•Šì•„ì„œ, ìƒˆë¡œ í›ˆë ¨í•˜ê³ , ì„ë² ë”©í™” í•˜ì—¬ ìƒì„±í–ˆë‹¤. (**create** and **train new embeddings)-**  [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) ì—¬ê¸°ì—ì„œ ì‰½ê²Œ ê°€ëŠ¥í•¨.

```python
# ë‹¤ìŒê³¼ ê°™ì´ 5 ê°€ì§€ special tokensì„ ì‚¬ìš©í•¨:
# - <bos> the sequenceì˜ ì²˜ìŒì„ ê°€ë¥´í‚´
# - <eos> the sequenceì˜ ëì„ ê°€ë¥´í‚´
# - <speaker1> ìœ ì €ì˜ ë°œí™”(utterance) ì²«ë¶€ë¶„ì„ ê°€ë¥´í‚´
# - <speaker2> ì±—ë´‡ì˜ ë°œí™”(utterance) ì²«ë¶€ë¶„ì„ ê°€ë¥´í‚´
# - <pad> as a padding token to build batches of sequences
SPECIAL_TOKENS = ["<bos>", "<eos>", "<speaker1>", "<speaker2>", "<pad>"]

# We can add these special tokens to the vocabulary and the embeddings of the model:
tokenizer.set_special_tokens(SPECIAL_TOKENS)
model.set_num_special_tokens(len(SPECIAL_TOKENS))
```

```python
from itertools import chain

# Let's define our contexts and special tokens
persona = [["i", "like", "playing", "football", "."],
           ["i", "am", "from", "NYC", "."]]
history = [["hello", "how", "are", "you", "?"],
           ["i", "am", "fine", "thanks", "."]]
reply = ["great", "to", "hear"]
bos, eos, speaker1, speaker2 = "<bos>", "<eos>", "<speaker1>", "<speaker2>"

def build_inputs(persona, history, reply):

words, segments, position, sequence = build_inputs(persona, history, reply)

# >>> print(sequence)  # Our inputs looks like this:
# [['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'],
#  ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],
#  ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],
#  ['<speaker1>', 'great', 'to', 'hear', '<eos>']]

# Tokenize words and segments embeddings:
words = tokenizer.convert_tokens_to_ids(words)
segments = tokenizer.convert_tokens_to_ids(segments)
```





## ğŸ‘‘ Multi-tasks losses

---

ìš°ë¦¬ëŠ” ì´ì œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  í›ˆë ¨ ì…ë ¥ì„ êµ¬ì¶•í–ˆìœ¼ë©°, ë‚¨ì€ ê²ƒì€ íŒŒì¸ íŠœë‹ ì¤‘ì— ìµœì í™”í•  ì†ì‹¤ì„ ì„ íƒí•˜ëŠ” ê²ƒë¿ì´ë‹¤

> ë¬¸ì¥ ì˜ˆì¸¡(Next-sentence prediction)ì„ ìœ„í•´ ì–¸ì–´ì™€ ê²°í•©ëœ multi-task lossì„ ì‚¬ìš©í–ˆë‹¤.

ë¬¸ì¥ ì˜ˆì¸¡ ëª©í‘œëŠ” BERT ì‚¬ì „í•™ìŠµ ë¶€ë¶„ì´ë‹¤. ê·¸ê²ƒì€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ *distactor*(ì •ë‹µ ì´ì™¸ì˜ ì„ íƒì§€)ë¥¼ ì¶”ì¶œí•˜ê³ , ì…ë ¥ ì‹œí€€ìŠ¤ê°€ gold reply(?)ë¡œ ëë‚˜ëŠ”ì§€ ì•„ë‹ˆë©´ distactorë¡œ ëë‚˜ëŠ”ì§€ êµ¬ë³„í•˜ê¸° ìœ„í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬ì„±ëœë‹¤. Local ë¬¸ë§¥(Context) ì´ì™¸ì—, Global ì„¸ê·¸ë¨¼íŠ¸ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ëŠ” ëª¨ë¸ì„ êµìœ¡í•œë‹¤.

![https://miro.medium.com/max/5326/1*945IpgUS9MGLB6gchoQXlw.png](https://miro.medium.com/max/5326/1*945IpgUS9MGLB6gchoQXlw.png)

Multi-Task Training ëª©ì  - ëª¨ë¸ì€ ì–¸ì–´ ëª¨ë¸ë§ ì˜ˆì¸¡ì„ ìœ„í•´ ë‘ ê°œì˜ í—¤ë“œë¥¼ ì œê³µ(ì˜¤ë Œì§€), ì˜ˆì¸¡ ë¬¸ì¥ ë¶„ë¥˜ê¸°(íŒŒë‘ìƒ‰)

### Total LossëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. **language modeling loss** ê³¼ **next-sentence prediction lossë¥¼ í†µí•´(ì´í•´ ì•ˆë¨)**

- **Language modeling:** we project the hidden-state on the word embedding matrix to get logits and apply a cross-entropy loss on the portion of the target corresponding to the gold reply (green labels on the above figure).
- **Next-sentence prediction:** we pass the hidden-state of the last token (the end-of-sequence token) through a linear layer to get a score and apply a cross-entropy loss to classify correctly a gold answer among distractors.



## ğŸ‘» Decoder ì„¸íŒ…

ì–¸ì–´ ìƒì„±ì„ ìœ„í•´ì„œëŠ” **greedy-decoding** ê³¼ **beam-search** ë°©ë²•ì„ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤. 

**Greedy-decoding**ì€ ë¬¸ì¥ì„ ë§Œë“œëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì´ë‹¤. ë§¤ ë‹¨ê³„ë§ˆë‹¤ ìˆœì„œì˜ ë í† í°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëª¨ë¸ì— ë”°ë¼ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ìŒ í† í°ì„ ì„ íƒí•œë‹¤. **Greedy-decoding**ì˜ í•œ ê°€ì§€ ìœ„í—˜ì€ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ì€ í† í°ì´ ë‚®ì€ í† í° ë’¤ì— ìˆ¨ì–´ ìˆë‹¤ê°€ ë†“ì¹  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

**Beam-search**ëŠ” ë‹¨ì–´ë³„ë¡œ êµ¬ì„±í•˜ëŠ” ëª‡ ê°€ì§€ ê°€ëŠ¥í•œ ìˆœì„œì˜ Beam(íŒŒë¼ë¯¸í„°: ë„ˆë¹„ì˜ ìˆ˜)ë¥¼ ìœ ì§€í•¨ìœ¼ë¡œì¨ ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ë ¤ê³  ë…¸ë ¥í•œë‹¤. ê·¸ ê³¼ì •ì´ ëë‚˜ë©´ Beam ì¤‘ì—ì„œ ê°€ì¥ ì¢‹ì€ ë¬¸ì¥ì„ ê³ ë¥¸ë‹¤. ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ ë¹” ê²€ìƒ‰ì€ ëŒ€í™” ìƒìë¥¼ í¬í•¨í•œ ê±°ì˜ ëª¨ë“  ì–¸ì–´ ìƒì„± ì‘ì—…ì—ì„œ í‘œì¤€ ë””ì½”ë”© ì•Œê³ ë¦¬ì¦˜ì´ì˜€ë‹¤.

### ê¸°ì¡´ ë¬¸ì œì 

ìµœê·¼ ë…¼ë¬¸(Ari Holtzman et al.)ì— ë”°ë¥´ë©´ "ë¹” ê²€ìƒ‰ê³¼ íƒìš•ìŠ¤ëŸ¬ìš´ í•´ë…ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ë¶„í¬ëŠ” ì¸ê°„ì´ ë§Œë“  í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ë¶„í¬ì™€ ë§¤ìš° ë‹¤ë¥´ë‹¤." ë¼ê³  í•œë‹¤. ë¶„ëª…íˆ ë¹” ê²€ìƒ‰ê³¼ íƒìš•ìŠ¤ëŸ¬ìš´ í•´ë…ì€ ëŒ€í™” ì‹œìŠ¤í…œì˜ ë§¥ë½ì—ì„œ [7, 8]ì—ì„œë„ ì–¸ê¸‰ë˜ì—ˆë“¯ì´ ì¸ê°„ ë°œí™”ì˜ ë¬¸ì ë¶„í¬ ì¸¡ë©´ì„ ì¬í˜„í•˜ì§€ ëª»í•œë‹¤.

![https://miro.medium.com/max/2830/1*yEX1poMDsiEBisrJcdpifA.png](https://miro.medium.com/max/2830/1*yEX1poMDsiEBisrJcdpifA.png)

*ì™¼ìª½: ì‚¬ëŒì´ ìƒì„±í•œ í† í°ê³¼ GPT-2ë¥¼ ì‚¬ìš©í•œ ë¹” ê²€ìƒ‰ì— í• ë‹¹ëœ í™•ë¥ (ë¹” ê²€ìƒ‰ìœ¼ë¡œ ì¬í˜„ë˜ì§€ ì•Šì€ ì¸ê°„ í…ìŠ¤íŠ¸ì˜ ê°•í•œ ì°¨ì´ë¥¼ ì°¸ê³ ) ì˜¤ë¥¸ìª½: ì¸ê°„ê³¼ ê¸°ê³„ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ N-ê·¸ë¨ ë¶„í¬(Greedy/Beam Search).*

### í•´ê²°ì±…

í˜„ì¬ Beam Search/Greedy ë””ì½”ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì€ **top-k**ì™€ **nuclear**(ë˜ëŠ” **top-p**) ìƒ˜í”Œë§ì´ë‹¤. ì´ ë‘ ë°©ë²•ë¡ ì€ ë¶„í¬ë¥¼ í•„í„°ë§í•œ í›„ ë‹¤ìŒ í† í° ë¶„í¬ì—ì„œ í‘œë³¸ì„ ì¶”ì¶œí•˜ì—¬ ëˆ„ì  í™•ë¥ ì´ ì„ê³„ê°’(nucleus/top-p) ë³´ë‹¤ ë†’ì€ ìˆëŠ” ìƒìœ„ í† í°(top-k) ë˜ëŠ” ìƒìœ„ í† í°ë§Œ ìœ ì§€í•˜ëŠ” ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ, **top-k** ì™€ **nucleus/top-p** ìƒ˜í”Œë§ì„ ë””ì½”ë”ë¡œ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì • í•˜ì˜€ë‹¤.



## ê²°ë¡ 

ë³¸ í¬ìŠ¤íŒ…ì—ì„œ ì„¤ëª… í•œ ê²ƒì²˜ëŸ¼, Hugging Faceì—ì„œëŠ” **Conversational AI**ë¥¼ êµ¬í˜„ í•˜ê¸° ìœ„í•´ ëŒ€ìš©ëŸ‰ ì–¸ì–´ ëª¨ë¸(large-scale language model)ì¸ OpneAIì˜ **GPT-2**ë¥¼ ì‚¬ìš©í–ˆë‹¤

ë³¸ í”„ë¡œì íŠ¸ì˜ ë°ëª¨ì™€ ìì„¸í•œ ì½”ë“œëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ ì°¾ì•„ ë³¼ ìˆ˜ ìˆë‹¤.

- [ë°ëª¨](http://convai.huggingface.co/)
- [Pre-trained ëª¨ë¸](https://github.com/huggingface/transfer-learning-conv-ai)



## References

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#c7f2) *Importance of a Search Strategy in Neural Dialogue Modelling* by Ilya Kulikov, Alexander H. Miller, Kyunghyun Cho, Jason Weston (http://arxiv.org/abs/1811.00907)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#5aac) *Correcting Length Bias in Neural Machine Translation* by Kenton Murray, David Chiang (http://arxiv.org/abs/1808.10006)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#5aac) *Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation* by Yilin Yang, Liang Huang, Mingbo Ma (https://arxiv.org/abs/1808.09582)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#97f9) *Hierarchical Neural Story Generation* by Angela Fan, Mike Lewis, Yann Dauphin (https://arxiv.org/abs/1805.04833)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#97f9) *Language Models are Unsupervised Multitask Learners* by Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever (https://openai.com/blog/better-language-models/)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#6ea4) *The Curious Case of Neural Text Degeneration* by Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi (https://arxiv.org/abs/1904.09751)

- *Retrieve and Refine: Improved Sequence Generation Models For Dialogue* by Jason Weston, Emily Dinan, Alexander H. Miller (https://arxiv.org/abs/1808.04776)

- [^](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#6ea4) *The Second Conversational Intelligence Challenge (ConvAI2)* by Emily Dinan et al. (https://arxiv.org/abs/1902.00098)